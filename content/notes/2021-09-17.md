+++
title = "2021-09-17"
author = ["Leobardo Argüelles"]
draft = false
+++

## Predicado II {#predicado-ii}

Comprobación directa.

Suponga \\(\exists A^{-1}\\) y dado \\(Ax=0 \therefore x=0\\) (vector columna)

\begin{align}
P\_1 &: \exists A^{-1}\\\\
P\_2 &: Ax=0\\\\
P\_3 &: A^{-1}Ax=A^{-1}0\\\\
P\_4 &: A^{-1}A=I\\\\
P\_5 &:Ix=A^{-1}0\\\\
P\_6 &:Ix=x\\\\
P\_7 &:x=0
\end{align}

Donde _x_ será un vector columna de tamaño n.


## Predicado III {#predicado-iii}

Comprobación directa.

Si \\(\lvert A \rvert \neq 0 \therefore \exists A^{-1}\\).


## Predicado IV {#predicado-iv}

A **no** posee dos o más matrices inversas (solo tiene una).

Comprobación por contradicción:

Premisa: Suponga que B y C son matrices inversas de \\(A \ni B \neq C\\).

\begin{align\*}
B&AC\\\\
(BA)C &= B(AC)\\\\
IC &= BI\\\\
C &= B
\end{align\*}


## Predicado V {#predicado-v}

Suponga una matriz diagonal.

\begin{equation\*}
A =
\begin{bmatrix}
a\_{11}\ \phantom{x}\ \phantom{x}\ \phantom{x}\\\\
\phantom{x}\ a\_{22}\ \phantom{x}\ \phantom{x}\\\\
\phantom{x}\ \phantom{x}\ \dots \ \phantom{x}\\\\
\phantom{x}\ \phantom{x}\ \phantom{x}\ a\_{nn}
\end{bmatrix}
\therefore
\ A^{-1} =
\begin{bmatrix}
\frac{1}{a\_{11}}\ \phantom{x}\ \phantom{x}\ \phantom{x}\\\\
\phantom{x}\ \frac{1}{a\_{22}}\ \phantom{x}\ \phantom{x}\\\\
\phantom{x}\ \phantom{x}\ \dots\ \phantom{x}\\\\
\phantom{x}\ \phantom{x}\ \phantom{x}\ \frac{1}{a\_{nn}}\end{bmatrix}
\end{equation\*}


## Predicado VI {#predicado-vi}

Suponga A, B matrices de (nxn) talque la inversa de A es \\(A^{-1}\\) y la
inversa de B es \\(B^{-1}\\). Entonces vale: \\((AB)^{-1} = B^{-1}A^{-1}\\)

Comprobación por construcción.

\begin{align\*}
(AB)\\\\
(AB)^{-1}\\\\
ABB^{-1}A^{-1} = I\\\\
A(BB^{-1})A^{-1} = I\\\\
AIA^{-1} =I\\\\
AA^{-1} = I\\\\
I=I\\\\
\end{align\*}


## Metodo de Gauss-Jordan para estimar la matriz inversa {#metodo-de-gauss-jordan-para-estimar-la-matriz-inversa}

Para una matriz A, se aumenta con la matriz identidad: [AI]

Ejemplo:

\begin{equation\*}
A=
\begin{bmatrix}
\ \ 2\ \ &\llap{$-$}1\ \ &0\\\\
\ \ \llap{$-$}1\ \ &2\ \ &\llap{$-$}1\\\\
\ \ 0\ \ &\llap{$-$}1\ \ &2
\end{bmatrix}
\end{equation\*}

Matriz aumentada [AI]:

\begin{equation\*}
[AI]=
\begin{bmatrix}
\ \ 2\ \ &\llap{$-$}1\ \ &0\ \ 1\ \ 0\ \ 0\\\\
\ \ \llap{$-$}1\ \ &2\ \ &\llap{$-$}1\ \ 0\ \ 1\ \ 0\\\\
\ \ 0\ \ &\llap{$-$}1\ \ &2\ \ 0\ \ 0\ \ 1\\\\
\end{bmatrix}
\end{equation\*}

Planteado esto, se aplican todas las matrices elementales necesarias
para transformar A en I.
Al final, cuando A se haya convertido en I, la sección que correspondía
a I en la matriz aumentada original corresponderá a la matriz inversa.


## Inversa de matriz elemental {#inversa-de-matriz-elemental}

Nada más se invierte el signo del multiplicador, y queda en la misma
posición.

Ejemplo:

\begin{equation\*}
E=
\begin{bmatrix}
1\ &0\ &0\\\\
-5\ &1\ &0\\\\
0\ &0\ &1
\end{bmatrix} \\; \\;
E^{-1}=
\begin{bmatrix}
1\ &0\ &0\\\\
5\ &1\ &0\\\\
0\ &0\ &1
\end{bmatrix}
\end{equation\*}


## Calcular determinante a partir de U {#calcular-determinante-a-partir-de-u}

Si tenemos una matriz A, y la llevamos a la forma U,
entonces la determinante se puede obtener multiplicando todos los
elementos de la diagonal.


## Calcular matriz inversa usando determinantes y cofactores {#calcular-matriz-inversa-usando-determinantes-y-cofactores}

\begin{equation\*}
A^{-1} = \frac{1}{\lvert A \rvert}(cofactores\ A)
\end{equation\*}


## Más nota {#más-nota}

-   Si A es simétrica, entonces su inversa es simétrica.


## Matriz tridiagonal (AKA matriz rala) {#matriz-tridiagonal--aka-matriz-rala}

Tiene la diagonal al centro, y otras 2 diagonales adyacentes. Todo lo
demás son ceros.

_Nota: Una matriz rala es una matriz que tiene más ceros que valores_
_Entonces, hablando de recursos computacionales, podemos comprimir_
_esa matriz, para no usar tanto espacio. La excepción es si vamos_
_a calcular su inversa, pues será densa._

Ejemplo:

\begin{equation\*}
\begin{bmatrix}
1\ 2\ 0\ 0\ 0\ 0\\\\
3\ 1\ 2\ 0\ 0\ 0\\\\
0\ 3\ 1\ 2\ 0\ 0\\\\
0\ 0\ 3\ 1\ 2\ 0\\\\
0\ 0\ 0\ 3\ 1\ 2\\\\
0\ 0\ 0\ 0\ 3\ 1\\\\
\end{bmatrix}
\end{equation\*}


### Matriz tridiagonal dominante {#matriz-tridiagonal-dominante}

Definición formal:
El valor absoluto del elemento en \\(a\_{ii}\\) es mayor que lo suma total
de los demás elementos de su línea. (Nota: \\(a\_{ii}\\) es un elemento de
la diagonal).

\begin{equation\*}
\lvert a\_{ii} \rvert > \sum\_{j \neq i} \lvert a\_{ij} \rvert
\end{equation\*}


### Propiedades {#propiedades}

La inversa de una matriz tridiagonal será densa, por lo que hablando de
progra y recursos, si vamos a sacar la inversa, es importante reservar
suficiente espacio para todos esos elementos.


## Matriz densa {#matriz-densa}

Es una matriz que casi no tiene ceros, tiene más valores que ceros.


## Conocimiento previo {#conocimiento-previo}

Para \\(a, b \in \mathbb{R}\\) con \\(a, b \neq 0\\). \\(\exists a^{-1} = \frac{1}{a}\\), \\(b^{-1} = \frac{1}{b}\\)
Este predicado es verdadero.

Sabiendo esto, suponga el predicado: \\((a+b)^{-1} = \frac{1}{(a+b)}\\)
Este va ser falso, y lo comprobamos con contraposición:

\begin{align\*}
a=2 \\; &y \\; b=-2\\\\
\\\\
a^{-1}=\frac{1}{2} \\; &y \\; b^{-1}=-\frac{1}{2} \\\\
\\\\
(a+b)^{-1} &= \frac{1}{0}
\end{align\*}

Como división entre 0 es indefinida, entonces el predicado no se sostiene.


## Tipos de comprobación {#tipos-de-comprobación}

-   Directa
    Se hilan varios predicados hasta comprobar que, con todos esos predicados
    juntos, se demuestra el predicado original.
-   Por contradicción
    Se "invierte" el predicado que se quiere demostrar. Por ejemplo:
    "No existe x" -> "Existe x". Y se comprueba que el segundo es erróneo.
